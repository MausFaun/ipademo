{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/PLA.png\" alt=\"Drawing\" style=\"width: 600px;\" align=\"left\"/> \n",
    "\n",
    "<br clear=\"all\" />\n",
    "\n",
    "<h1 style=\"color: rgb(41,186,116)\" face=\"Trebuchet MS\">Classifying Handwritten Digits with Neural Networks</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: rgb(41,186,116)\" face=\"Trebuchet MS\">Learning Objectives:</h3>\n",
    "<font size=\"2\" style=\"color: rgb(87,87,87)\" face=\"Trebuchet MS\">\n",
    "<ul>\n",
    "  <li>Train a neural network to classify handwritten digits from the classic MNIST data set</li>\n",
    "  <li>Visualize the neural network, its weights, the training process and its performance</li>\n",
    "  <li>Compare the performance of your neural network classification with the other partcipants in the class (<b>the best performing model is even awarded a small price, so do your best!</b>)</li>\n",
    "</ul>\n",
    "</font>\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: rgb(41,186,116)\" face=\"Trebuchet MS\">Setup</h3>\n",
    "\n",
    "<font size=\"2\" style=\"color: rgb(87,87,87)\" face=\"Trebuchet MS\">First, let's import Googles' powerful TensorFlow libary and the corresponding Python wrapper, Keras. We are also going to need some additional utilities. \n",
    "To verify that everything works as expected, we check the TensorFlow version used.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TensorFlow and Keras\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "# Additional utilities\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from IPython.display import display\n",
    "# Check the TensorFlow version\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: rgb(41,186,116)\" face=\"Trebuchet MS\">Load Data</h3>\n",
    "\n",
    "<font size=\"2\" style=\"color: rgb(87,87,87)\" face=\"Trebuchet MS\">Next, let's download the data set and load the MNIST data. We split the data in a 'train' and 'test' subset evenly.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"2\" style=\"color: rgb(87,87,87)\" face=\"Trebuchet MS\">Each element in (x_train, y_train) represents one labeled example. Element 0 in y_train represents the label that a human rater has assigned for one handwritten digit. For example, if element 0 contains '5', then a human rater interpreted the handwritten character as the digit '5'. The ten digits 0-9 are each represented, with a unique class label for each possible digit. Thus, this is a multi-class classification problem with 10 classes.</font>\n",
    "\n",
    "<img src=\"img/MNIST-Matrix.png\" alt=\"Drawing\" style=\"width: 800px;\" align=\"middle\"/> \n",
    "\n",
    "<font size=\"2\" style=\"color: rgb(87,87,87)\" face=\"Trebuchet MS\">Each element in x_train contains 784 feature values, one per pixel for the 28Ã—28=784 pixel values. The pixel values are on a gray scale in which 0 represents white, 255 represents black, and values between 0 and 255 represent shades of gray. Most of the pixel values are 0; meaning just white space.\n",
    "<br clear=\"all\" />\n",
    "You may want to confirm that, so let's see what column 0 in x_train contains.</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Picture in postion 0\n",
    "plt.imshow(x_train[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font size=\"2\" style=\"color: rgb(87,87,87)\" face=\"Trebuchet MS\"> Additionally, let's check what the human rater interpreted the number as.</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label for the picture in postion 0\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"2\" style=\"color: rgb(87,87,87)\" face=\"Trebuchet MS\">Looks about right, wouldn't you agree?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: rgb(41,186,116)\" face=\"Trebuchet MS\">Preparing the training process</h3>\n",
    "\n",
    "<font size=\"2\" style=\"color: rgb(87,87,87)\" face=\"Trebuchet MS\">To make training a bit easier and enhance accuracy, we normalize train and test data from a scale of <font size=\"2\" style=\"color: rgb(41,186,116)\" face=\"Trebuchet MS\">0...255</font> (grayscale values) to <font size=\"2\" style=\"color: rgb(41,186,116)\" face=\"Trebuchet MS\">0...1</font>.</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tf.keras.utils.normalize(x_train, axis=1)\n",
    "x_test = tf.keras.utils.normalize(x_test, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: rgb(41,186,116)\" face=\"Trebuchet MS\">Constructing the neural network</h3>\n",
    "\n",
    "<font size=\"2\" style=\"color: rgb(87,87,87)\" face=\"Trebuchet MS\">Now we come to the intresting part; building the model. We are choosing the model type and the size of the input layer, in this case equal to the number of input features (784 pixels = 784 neurons).\n",
    "\n",
    "\n",
    "There are a lot of details here, but don't worry, you are not required to understand every aspect of it. For the nerds among us: We are building a sequential model, meaning that we do not allow for backwards propagation. Also, we flatten the input layer, because we have no multidimensional data to process. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a model type\n",
    "model = tf.keras.models.Sequential()\n",
    "# Add the input layer\n",
    "model.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: rgb(41,186,116)\" face=\"Trebuchet MS\">Recap: Neural network structure</h3>\n",
    "<br clear=\"all\" />\n",
    "<img src=\"img/network.png\" alt=\"Drawing\" style=\"width: 400px;\" align=\"left\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: rgb(41,186,116)\" face=\"Trebuchet MS\">Network depth</h3>\n",
    "\n",
    "<font size=\"2\" style=\"color: rgb(87,87,87)\" face=\"Trebuchet MS\">Now it is time to add some hidden layers to our network. \n",
    "Decide how many layers you deem appropriate for the task at hand.</font>\n",
    "\n",
    "> <font size=\"2\" style=\"color: rgb(231,28,87)\" face=\"Trebuchet MS\">Your input is required! Choose the number of hidden layers for your network. For performance reasons, the number should be between 1 and 8.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust the network depth\n",
    "%run scripts/network_depth.py\n",
    "depth = query_network_depth()\n",
    "display(depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: rgb(41,186,116)\" face=\"Trebuchet MS\">Layer width</h3>\n",
    "\n",
    "<font size=\"2\" style=\"color: rgb(87,87,87)\" face=\"Trebuchet MS\">Next, you need to configure the number of neurons per hidden layer.\n",
    "Decide how many neurons you deem appropriate for each of the hidden layers you constructed.</font>\n",
    "\n",
    "> <font size=\"2\" style=\"color: rgb(231,28,87)\" face=\"Trebuchet MS\">Your input is required! Choose the number of neurons for each layer.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust the network depth\n",
    "%run scripts/layer_width.py\n",
    "layer_width = query_layer_width(depth)\n",
    "for widget in layer_width:\n",
    "    display(widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"2\" style=\"color: rgb(87,87,87)\" face=\"Trebuchet MS\">The code below will add the number of layers you assigned to the network. \n",
    "Our network will employ dense layers, meaning that every neuron on a given layer is connected to all neurons on the previous layer and utilize a ReLu activation function. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the number of hidden layers choosen and XXX\n",
    "for x in layer_width:\n",
    "    model.add(tf.keras.layers.Dense(x.value, activation=tf.nn.relu))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"2\" style=\"color: rgb(87,87,87)\" face=\"Trebuchet MS\">Last but not least we are missing an output layer. Remember, this is classification problem. Our possible classification outputs are the numbers 0...9; so ten outputs. \n",
    "\n",
    "Therefore, our output layer requires 10 outputs as well!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the output layer\n",
    "model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"2\" style=\"color: rgb(87,87,87)\" face=\"Trebuchet MS\">We are finaly ready to compile the model.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: rgb(41,186,116)\" face=\"Trebuchet MS\">Training the model</h3>\n",
    "\n",
    "<font size=\"2\" style=\"color: rgb(87,87,87)\" face=\"Trebuchet MS\">Remember what you learned today; the training of neural networks is the adjustment of weights on connections between links. We are missing one last parameter: In how many iterations (epochs) would you like to train the model? \n",
    "(Be sensible about the number of epochs, models that have not finished training by the end of this session will not be included in the competition!)</font>\n",
    "\n",
    "> <font size=\"2\" style=\"color: rgb(231,28,87)\" face=\"Trebuchet MS\">Your input is required! Choose the number of epochs for training your model!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust the number of training epochs\n",
    "%run scripts/training_epochs.py\n",
    "epochs = query_training_epochs()\n",
    "display(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train the model\n",
    "history = model.fit(x_train, y_train, epochs=epochs.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"2\" style=\"color: rgb(87,87,87)\" face=\"Trebuchet MS\">All done! Let's visualize how the training went. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visulize training loss\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(history.epoch,history.history['loss'])\n",
    "plt.title('loss')\n",
    "\n",
    "# visualize training accuracy\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(history.epoch,history.history['acc'])\n",
    "plt.title('accuracy');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: rgb(41,186,116)\" face=\"Trebuchet MS\">Wrapping up</h3>\n",
    "<font size=\"2\" style=\"color: rgb(87,87,87)\" face=\"Trebuchet MS\">\n",
    "No more hiding! The next step scores your model. Classical measures for model performance are accuracy and loss. We'll use those along with the number of epochs you choose for training. \n",
    "\n",
    "Roughly speaking, the following rules apply:\n",
    "<ul>\n",
    "  <li>The <font size=\"2\" style=\"color: rgb(41,186,116)\" face=\"Trebuchet MS\">higher</font> your model accuracy, the better your model scores.</li>\n",
    "  <li>The <font size=\"2\" style=\"color: rgb(41,186,116)\" face=\"Trebuchet MS\">lower</font> the loss, the better your model scores. </li>\n",
    "  <li>The <font size=\"2\" style=\"color: rgb(41,186,116)\" face=\"Trebuchet MS\">lower</font> the number of epochs configured, the better your model scores.</li>\n",
    "</ul>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_loss, val_acc = model.evaluate(x_test, y_test)\n",
    "print('Your model achieved a loss score of {}'.format(val_loss))\n",
    "print('Your model had an accuracy of {}%'.format(val_acc))\n",
    "print('Your model was trained in {} epochs'.format(epochs.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: rgb(41,186,116)\" face=\"Trebuchet MS\">Closing notes</h3>\n",
    "<font size=\"2\" style=\"color: rgb(87,87,87)\" face=\"Trebuchet MS\">\n",
    "For those interested in what the model did wrong; the following code will show you a draw of 100 numbers where your model went wrong</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.predict_classes(x_test)\n",
    "\n",
    "test_wrong = [im for im in zip(x_test,results,y_test) if im[1] != im[2]]\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for ind, val in enumerate(test_wrong[:100]):\n",
    "    plt.subplots_adjust(left=0, right=1, bottom=0, top=1)\n",
    "    plt.subplot(10, 10, ind + 1)\n",
    "    im = 1 - val[0].reshape((28,28))\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    # True label\n",
    "    plt.text(0, 0, val[1], fontsize=14, color='red')\n",
    "    \n",
    "    # Estimated label\n",
    "    plt.text(8, 0, val[2], fontsize=14, color='blue')\n",
    "    plt.imshow(im, cmap='gray')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
